{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNW2D9/TojNLtE7JUOe6XvG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"lueo9zG1D0Y7","executionInfo":{"status":"error","timestamp":1679921743678,"user_tz":-120,"elapsed":1279906,"user":{"displayName":"Alberto DÃ­ez","userId":"14787414139684263049"}},"outputId":"8f167526-ffb8-4bce-b69f-1200775096e2"},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1e60cbc5f1f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mlocation_datetime_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocation_and_datetime_soup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Splitting time and location into two parts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         record = {\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;34m\"Organisation\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0morganisation_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;34m\"Location\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlocation_datetime_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;34m\"Datetime\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlocation_datetime_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'getText'"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n"," \n","# Find the last page number.\n","response = requests.get(\"https://nextspaceflight.com/launches/past/?search=\")\n","data = response.text\n","soup = BeautifulSoup(data, 'html.parser')\n","last_page_soup = soup.select_one('.mdc-button--raised:nth-of-type(2)')\n","last_page = int(last_page_soup.get('onclick').split('=')[2].split('&')[0])\n"," \n","final_data = []\n"," \n","for page_no in range(1, last_page+1):  # loop through every page.\n","    response = requests.get(f\"https://nextspaceflight.com/launches/past/?page={page_no}&search=\")\n","    data = response.text\n","    soup = BeautifulSoup(data, 'html.parser')\n","    missions_in_page = soup.select('h5')\n","    location_and_datetime_soup = soup.select('.mdl-card__supporting-text')  # no distinctive classes inside the page hence extracting it outside itself.\n","    mission_details_link_soup = soup.select('.mdc-button:first-child')\n"," \n","    for i in range(len(missions_in_page)):  # loop through all missions in one page.\n","        mission_details_link = mission_details_link_soup[i].get('onclick')[35:-1]\n","        response = requests.get(f\"https://nextspaceflight.com/launches/details/{mission_details_link}\")\n","        data = response.text\n","        soup = BeautifulSoup(data, 'html.parser')\n","        details_soup = soup.select_one('title')\n","        mission_status_soup = soup.select_one('.status')\n","        organisation_soup = soup.select_one('.a:first-child .mdl-cell:first-child')\n","        mission_data_soup = soup.select_one('.a:first-child .mdl-cell:nth-of-type(2)')\n","        mission_price_soup = soup.select_one('.a:first-child .mdl-cell:nth-of-type(3)')\n","        location_datetime_split = location_and_datetime_soup[i].get_text(strip=True, separator=\"#\").split('#')  # Splitting time and location into two parts.\n","        record = {\n","            \"Organisation\": organisation_soup.getText(),\n","            \"Location\": location_datetime_split[1],\n","            \"Datetime\": location_datetime_split[0],\n","            \"Details\": details_soup.getText(),\n","            \"Status\": mission_data_soup.getText(strip=True).split(': ')[1],\n","            \"Price\": mission_price_soup.getText(strip=True)[8:-8],\n","            \"Mission_status\": mission_status_soup.getText(strip=True)\n","        }\n","        final_data.append(record)\n"," \n","    for mission in final_data:  # convert price to float and replace bad price data with empty strings.\n","        try:\n","            mission['Price'] = float(mission['Price'])\n","        except ValueError:\n","            mission['Price'] = ''\n"," \n","print(final_data)\n"," \n","# create and add scraped data to a csv file.\n","pd.DataFrame(final_data).to_csv(\"mission_launches_updated.csv\")"]}]}